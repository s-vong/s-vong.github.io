---
layout: post
title: Selu 
categories: keras 
---

<h2>Introduction</h2>
<p>
In a previous blog post, I put off learning about SELU (<b>S</b>caled <b>E</b>xponential <b>L</b>inear <b>U</b>nit) because of the lengthy paper the activation function was published in. Instead, I found an <a href="https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9">article</a> by Elior Cohen who explains the concept well. So I will, in turn, try to summarize that information.
</p>

<h2>SELU</h2>
<p>
SELU is almost like ELU (Who would have thought?), but with a key difference. 
<br />
elu(x) = x if x >= 0 and (alpha)(e^x - 1) if x < 0
<br />
selu(x) = (lambda)(x) if x > 0 and (lambda)(alpha)e^x - (alpha) x <= 0. 
<br />
There are two fixed values (lambda and alpha) as opposed to ELU which only has lambda. Usually, the standard values for lambda and alpha will be close to 1. Therefore, it isn't much different to ELU. The difference is because of SELU's fixed values, they cannot be backpropagated. The reason why SELU is useful is for its self-normalizing properties. Thus, SELU goes well in hand with self-normalizing neural networks (SNN). 
<br />
The key features for SNN is the ability to train deep neural networks, strong regularization schemes and robust learning.
</p>

<h2>Conclusion</h2>
<p>
I should probably get used to reading papers published by researchers. In order to do this, I will need to brush up on the fundamentals of machine learning. In particular, the mathematical portion is where I need to start over again. My plan of action is going to be binging on linear algebra tutorials and Coursera lectures by Andrew Ng of Stanford University.
<br />
You can read more of SELU at the following link:  <a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>
</p>
